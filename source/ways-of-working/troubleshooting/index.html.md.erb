---
title: Troubleshooting issues
last_reviewed_on: 2021-03-25
review_in: 6 months
weight: 99
---

# <%= current_page.data.title %>

## Connecting to AKS Clusters

  - By Default, all developers have read access to non-prod AKS clusters and slightly higher privileges to their namespaces.
  - You can connect to AKS clusters using `az aks get-credentials`. Below are some handy commands,
    
    ```shell
    #Preview (only one cluster is active at a given time)
    az aks get-credentials --resource-group preview-00-rg --name preview-00-aks --subscription DCD-CFTAPPS-DEV
    az aks get-credentials --resource-group preview-01-rg --name preview-01-aks --subscription DCD-CFTAPPS-DEV
    
    #AAT
    az aks get-credentials --resource-group cft-aat-00-rg --name cft-aat-00-aks --subscription DCD-CFTAPPS-STG
    az aks get-credentials --resource-group cft-aat-01-rg --name cft-aat-01-aks --subscription DCD-CFTAPPS-STG
    

    #Perftest
    az aks get-credentials --resource-group cft-perftest-00-rg --name cft-perftest-00-aks --subscription DCD-CFTAPPS-TEST 
    az aks get-credentials --resource-group cft-perftest-01-rg --name cft-perftest-01-aks --subscription DCD-CFTAPPS-TEST 
    
    #ITHC
    az aks get-credentials --resource-group ithc-00-rg --name ithc-00-aks --subscription DCD-CFTAPPS-ITHC 
    az aks get-credentials --resource-group ithc-01-rg --name ithc-01-aks --subscription DCD-CFTAPPS-ITHC 
    
    #DEMO (only one cluster is active at a given time)
    az aks get-credentials --resource-group demo-00-rg --name demo-00-aks --subscription DCD-CFTAPPS-DEMO
    az aks get-credentials --resource-group demo-01-rg --name demo-01-aks --subscription DCD-CFTAPPS-DEMO
    
    #Prod (Requires additional permissions)
    
    az aks get-credentials --resource-group prod-00-rg --name prod-00-aks --subscription DCD-CFTAPPS-PROD
    az aks get-credentials --resource-group prod-01-rg --name prod-01-aks --subscription DCD-CFTAPPS-PROD
    
    ```
    Once you have logged in, you can switch between clusters using [kubectx](https://github.com/ahmetb/kubectx) or below kubectl commands:
    
    ```shell
    kubectl config use-context cft-perftest-00-aks
    kubectl config use-context cft-aat-00-aks
    ```

## Jenkins

### Cannot see my branch/PR or This project is currently disabled in Jenkins

  - Any branches which are also filed as PRs are not listed as a branch, they will only be listed in pull requests section.
  - Branch/ PR is not listed if its last commit creation date is older than 30 days.

### Helm Upgrade Failed, Helm Release timed out waiting for condition, Helm Release Failure
  
  - See [Connecting to AKS Clusters](#connecting-to-aks-clusters) and connect to the relevant cluster.
  - These errors usually mean your pods didn't start as expected in time. 
  - It could be that they are stuck in `Pending`, `ContainerCreating` status or might be failing to startup leading to `CrashLoopBackOff` status.
  - Follow [Debug Application Startup issues in AKS](#debug-application-startup-issues-in-aks) to troubleshoot further
  

## Debug Application Startup issues in AKS
 
- There could be many reasons why applications could fail to startup like :
    - A secret referred in helm chart is missing in keyvaults
    - Pod identity is not able to pull keyvault secrets due to missing permissions
    - There is not enough space in the cluster to fit in a new pod.
    - Pod is scheduled, but fails to pass readiness (`/health/readiness`) or  liveness (`/health/liveness`) checks.
- Below are some handy kubectl commands to debug the issues
    
    To check latest events on your namespace:
     
     ```shell
     kubectl get events -n <your-namespace>
     ```
     
     To check status of pods:
     
     ```shell
     kubectl get pods -n <your-namespace> | grep <helm-release-name>
     
     #Examples
     # kubectl get pods -n ccd | grep ccd-data-store-api
     # kubectl get pods -n ccd | grep pr-123
     ```
     
     To check status of a specific pod which is not running 
     
     ```shell
     kubectl describe pod <pod-name> -n <your-namespace>
     ```
     
     To check logs of pods which is not starting
     
     ```shell
     kubectl logs <pod-name> -n <your-namespace> 
     
     #To follow logs
     kubectl logs <pod-name> -n <your-namespace> -f
     
     # To check previous pod logs if its restarting
     kubectl logs <pod-name> -n <your-namespace> -p
     
     ```

## Flux / Gitops

   > Always check __why__ your release or pod has failed in the first instance.
   > Although you may have permissions to delete a helm release or pod in a non-production environment, use this privilege wisely as you could be _hiding a potential bug_ which could also _occur in production_. 

### New image is not updated in cluster.
   
   - Start with checking [cnp-flux-config](https://github.com/hmcts/cnp-flux-config) to make sure flux has updated/ committed the image. If it hasn't committed to Github, see [ Flux did not commit new image to Github](#flux-did-not-commit-new-image-to-github).
   - If flux has committed the new image to Github, check if the `HelmRelease` has been updated by Flux. Run below command and check that the image tag has been updated in the output
    
    ```shell
      kubectl get hr -n <your-namespace> <your-helm-release-name> -o yaml
    ```
   - If Image is not updated in above, [check flux logs](#flux-did-not-commit-new-image-to-github).
   - If the image tag is updated and still application pods are not deployed, see [Updated HelmRelease is not deployed](#updated-helmrelease-is-not-deployed) 

### Flux did not commit new image to Github.

   - Flux keeps polling ACR for new images, but it should generally update the new image in 10 minutes.
   - Look at flux logs to see if there are any errors specific to your release  
      
     ```shell
     kubectl logs -n admin -l app=flux --tail=100 | grep <Release Name>
     ```
       
   - If you see an error which says `failed to resolve reference` or `not found` on a particular image, it means the current image deployed is deleted from the container registry and you need to manually update to latest image in cnp-flux-config to fix your helm release.
   - If you don't see any error logs for your specific release, check if there are any errors that could be blocking flux.
       
     ```shell
     kubectl logs -n admin -l app=flux --tail=100
     ```
        
   - If there are no errors, it is likely that flux pod is stuck and a symptom for it is not seeing a log containing `cmd="kubectl apply -f -` in flux logs. You should [ask for help from platops](../asking-for-help)) to get it sorted.
      
      ```shell
      kubectl logs -n admin -l app=flux --tail=100
      ```
  
### Updated HelmRelease is not deployed
    
   - Helm operator queues all the updates, so it could take up to 20 minutes sometimes to be picked up.
   - Look at helm operator logs to see if there are any errors specific to your helm release
   
      ```shell
      kubectl logs -n admin -l app=helm-operator --tail=1000 | grep <Release Name>
      ```
   - If you see any errors like, `status 'pending-install' of release does not allow a safe upgrade"`. You need to delete `HelmRelease` for fixing this, request platops for help if you do not have permissions.
     
      ```shell
      kubectl delete hr <helm-release-name> -n <namespace>
      ```
   - In most cases, helm release gets timed out with an error in log similar to ` failed: timed out waiting for the condition`. This usually means application pods didn't startup in time and you need to look at your pods to know more.
       
     Check the latest status on helm release and if it has already been rolled back to previous release.
     
     ```shell
     kubectl describe hr <helm-release-name> -n <your-namespace>
     ```
   - If you are looking at pods after a long time, `HelmRelease` might have been rolled back and you won't have failed pods. Easiest way is to add a simple change like a dummy environment variable in flux-config to re-trigger the release and debug the issue when it occurs.
 
   - If your old pods are still running when you check, follow [Debug Application Startup issues in AKS](#debug-application-startup-issues-in-aks) to troubleshoot further.
    
###  Change in flux config is not applied to cluster.

   - Check if the `HelmRelease` has been updated with your change. Run below command and check that the image tag has been updated in the output.
   
     ```shell
     kubectl get hr -n <your-namespace> <your-helm-release-name> -o yaml
     ```

   - If Changed is not updated in above, [check flux logs](#flux-did-not-commit-new-image-to-github).
   
   - If the change is updated, but still not reflecting on your pods/ application. see [Helm operator logs](#updated-helmrelease-is-not-deployed)
