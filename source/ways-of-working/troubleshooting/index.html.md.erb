---
title: Troubleshooting issues
last_reviewed_on: 2022-08-05
review_in: 6 months
weight: 99
---

# <%= current_page.data.title %>
## GitHub

### Adding a new user on GitHub

 - See [GitHub Onboarding](/onboarding/team/github.html#access) for how to get access
 - Make sure the user has accepted the invite received on the email setup in Github or by visiting [github.com/hmcts](https://github.com/hmcts).

### Creating a new GitHub repository

See [create a GitHub repository](/ways-of-working/new-component/github-repo.html#)

### Access to GitHub Repo

 - All repositories should be administered by the team who owns it.
 - Access should be managed through GitHub teams and not individuals.
 - If no one from your team have access, follow [asking for help](/ways-of-working/asking-for-help/#asking-for-help) to request org admins in one of the community/ support channels.

## Jenkins

### Jenkins is unavailable

  - Check if there is a planned outage in [#cloud-native-announce](https://hmcts-reform.slack.com/archives/CA4F2MAFR).
  - Please note Jenkins could be temporarily unavailable while rolling out a change, give it a few minutes before you raise an issue.

### Cannot login to Jenkins

  - Login to Jenkins is managed by Azure AD, please make sure user is added to right [Azure AD groups](/onboarding/person/#azure-ad-groups)

### You are now logged out of Jenkins/ Infinite loop when trying to login to Jenkins

  - Please try clearing browser cookies on [login.microsoft.com](https://login.microsoftonline.com/)

### Cannot see my new repo in Jenkins org / Dashboard

  - See [Jenkins Onboarding section](/onboarding/team/jenkins.html#jenkins) to add your app to Jenkins Org / Dashboard

### Cannot find default branch

You may get the error below, when the pipeline has not ran on the master/main branch first.

When you run it on the master/main branch it will setup the default branch and then the pull request build will start working.

```
[2021-09-16T10:59:54.154Z] Execution failed for task ':sonarqube'.
[2021-09-16T10:59:54.154Z] > Could not find a default branch to fall back on.
```
### Cannot see my branch/PR or This project is currently disabled in Jenkins

  - Any branches which are also filed as PRs are not listed as a branch, they will only be listed in pull requests section.
  - Branch/ PR is not listed if its last commit creation date is older than 30 days.

### Build / Docker Build / Unit Test failure

  - If your build is failing in these stages, it's most likely to fail in your local as well. Look at the first line of the Jenkins step that fails and try run the same command Jenkins is running.
  - Try on a colleague's machine as you might have cached something locally.

### Sonar scan timeout

  - Please see [sonarcloud status](https://sonarcloud.statuspage.io/) for any known issues with sonar cloud.
  - Remember that Platform Operation do not maintain SonarCloud, issues are usually discussed on community forums.

### Helm Upgrade Failed, Helm Release timed out waiting for condition, Helm Release Failure
  
  - See [Connecting to AKS Clusters](#connecting-to-aks-clusters) and connect to the relevant cluster.
  - These errors usually mean your pods didn't start as expected in time. 
  - It could be that they are stuck in `Pending`, `ContainerCreating` status or might be failing to startup leading to `CrashLoopBackOff` status.
  - Follow [Debug Application Startup issues in AKS](#debug-application-startup-issues-in-aks) to troubleshoot further

### Jenkins managed helm releases / pods are automatically deleted

  - To maintain the health of the cluster, it is important to cleanup unwanted pods regularly.
  - Helm release is cleared for PRs which are merged or closed.
  - Helm release of PRs raised by dependency bots (based on `dependencies` label) is cleared once the functional tests pass.
  - Helm release on AAT Staging is also cleared once functional tests pass.
  - For optimal usage, You can also configure your pipeline to [clear Helm releases on successful build](https://github.com/hmcts/cnp-jenkins-library#clear-helm-release-on-successful-build).
  - A scheduled pipeline runs every hour to clear any helm releases which are not updated in last 3 days. Teams can do more frequent cleanup by overriding in the [cleanup script](https://github.com/hmcts/cnp-aks-pipelines/blob/0fe733120f78b6dabcdd5895bb16134085631842/scripts/delete-inactive-helm-releases.sh#L5)

### Smoke / Functional test failure

  - Jenkins only sets secrets as environment variables and runs the `gradle` / `yarn` task to run tests.
  - Access the URL on VPN and try running tests manually using the `TEST_URL` printed in the logs.
  - You can also run tests locally by setting the required secrets while on the VPN.
  - To add additional logging, see [Example config](#example-2).

### Terraform/ Build Infrastructure failure

  - It is important that you need to review your plan in a pull request before applying it to an environment.
  - Please check if its an intermittent failure with Azure as a retry could fix it.
  - Also, see if there are any open issues/discussions on community channels for the failure.
  - There could be open GitHub issues on terraform/ azurerm, so googling it could help as well.

### Using branches to troubleshoot issues

If your pipeline is throwing an error, you may be able to more easily troubleshoot the issue by using a branch in the cnp-jenkins-library repo.

#### Example 1
Gradle is failing because a plugin cannot be found in artifactory. You've checked the plugin exists and there are no typos in your code.
You can check if the issue lies with artifactory by temporarily bypassing it using a [branch](https://github.com/hmcts/cnp-jenkins-library/blob/073ad8587b7281d62bac705ed984e739a0911c83/resources/uk/gov/hmcts/gradle/init.gradle#L3).

And then referencing that branch within your repo's [Jenkinsfile](https://github.com/hmcts/sds-toffee-recipes-service/pull/12/files)

In this example, the issue is network related. This may be down to routing in Azure or traffic being blocked by a firewall.

#### Example 2
Gradle is failing a functional test. To help get more information on why, you could update the Gradle logging level in a branch of [cnp-jenkins-library](https://github.com/hmcts/cnp-jenkins-library/blob/cfb31f3a2699b2a1dafd66fed0b525ae145d627d/src/uk/gov/hmcts/contino/GradleBuilder.groovy#L62)

And then reference this branch within your repo's [Jenkinsfile](https://github.com/hmcts/document-management-store-app/blob/646593336377fd59112b0b6c84fd223d0cb7832c/Jenkinsfile_CNP#L12)

## Debug Application Startup issues in AKS
 
- There could be many reasons why applications could fail to startup like :
    - A secret referred in helm chart is missing in keyvaults
    - Pod identity is not able to pull keyvault secrets due to missing permissions
    - There is not enough space in the cluster to fit in a new pod.
    - Pod is scheduled, but fails to pass readiness (`/health/readiness`) or  liveness (`/health/liveness`) checks.
    - A misconfigured environment variable, example - incorrect URL of a dependent service.

- Below are some handy kubectl commands to debug the issues
    
    To check latest events on your namespace:
     
     ```shell
     kubectl get events -n <your-namespace>
     ```
     
     To check status of pods:
     
     ```shell
     kubectl get pods -n <your-namespace> | grep <helm-release-name>
     
     #Examples
     # kubectl get pods -n ccd | grep ccd-data-store-api
     # kubectl get pods -n ccd | grep pr-123
     ```
     
     To check status of a specific pod which is not running 
     
     ```shell
     kubectl describe pod <pod-name> -n <your-namespace>
     ```
     
     To check logs of pods which is not starting
     
     ```shell
     kubectl logs <pod-name> -n <your-namespace> 
     
     #To follow logs
     kubectl logs <pod-name> -n <your-namespace> -f
     
     # To check previous pod logs if its restarting
     kubectl logs <pod-name> -n <your-namespace> -p
     
     ```

## Flux / Gitops

   > Always check __why__ your release or pod has failed in the first instance.
   > Although you may have permissions to delete a helm release or pod in a non-production environment, use this privilege wisely as you could be _hiding a potential bug_ which could also _occur in production_. 

### Latest image is not updated in cluster

- Start with checking [cnp-flux-config](https://github.com/hmcts/cnp-flux-config) to make sure flux has updated/ committed the image. 
- If image hasn't been committed to Github, see [ Flux did not commit latest image to Github](#flux-did-not-commit-latest-image-to-github).
- If flux has committed the new image to Github, check if the `HelmRelease` has been updated by Flux. Run below command and check that the image tag has been updated in the output

    ```shell
    kubectl get helmreleases.helm.toolkit.fluxcd.io -n <your-namespace> <your-helm-release-name> -o yaml
    ```
- If Image is not updated in above, [Change in git is not applied to cluster](#change-in-git-is-not-applied-to-cluster).
- If the image tag is updated and still application pods are not deployed, see [Updated HelmRelease is not deployed to cluster](#updated-helmrelease-is-not-deployed-to-cluster)

### Flux did not commit latest image to Github

   - Image automation is run from management cluster (CFTPTL). Please login to cftptl cluster before further troubleshooting.
   - Image reflector controller keeps polling ACR for new images, but it should generally update the new image in 10 minutes.
   - Check status of `imagerepositories` and verify the last scan.

    ```shell
    kubectl get imagerepositories -n flux-system  <repository name(usually helm release name)> 
    ```
   - If the last scan doesn't update, check image reflector controller logs to see if there any logs related to the helm repo.

    ```shell
    kubectl logs -n flux-system -l app=image-reflector-controller --tail=-1
    # search for specific image
    kubectl logs -n flux-system -l app=image-reflector-controller --tail=-1 | grep <Release Name>
    ```
   - If the last scan is latest, check `imagepolicy` status to verify that the image returned matches the expectation.

    ```shell
    kubectl get imagepolicies -n flux-system <policy name(usually helm release name)>
    ```
   - If it doesn't match the expected tag, verify image reflector controller logs as described above.
   - If the `imagepolicy` object returned shows the expected image, but it didn't commit to Github, check image automation controller logs.

    ```shell
    kubectl logs -n flux-system -l app=image-automation-controller
    # search for specific image
    kubectl logs -n flux-system -l app=image-automation-controller | grep <Release Name>
    ```

### Updated HelmRelease is not deployed to cluster

   - Helm operator queues all the updates, so it could take up to 20 minutes sometimes to be picked up.
   - Check HelmRelease status to see the status.

    ```shell
    kubectl get helmreleases.helm.toolkit.fluxcd.io -n <namespace> <Release Name>
    ```
   - Look at helm operator logs to see if there are any errors specific to your helm release

    ```shell
    kubectl logs -n flux-system -l app=helm-controller --tail=1000 | grep <Release Name>
    ```
   - If you see any errors like, `status 'pending-install' of release does not allow a safe upgrade"`. You need to delete `HelmRelease` for fixing this, request platops for help if you do not have permissions.

    ```shell
    kubectl delete helmreleases.helm.toolkit.fluxcd.io <helm-release-name> -n <namespace>
    ```
   - In most cases, helm release gets timed out with an error in log similar to ` failed: timed out waiting for the condition`. This usually means application pods didn't startup in time and you need to look at your pods to know more.

     Check the latest status on helm release and if it has already been rolled back to previous release.

    ```shell
    kubectl describe helmreleases.helm.toolkit.fluxcd.io <helm-release-name> -n <your-namespace>
    ```
   - If you are looking at pods after a long time, `HelmRelease` might have been rolled back and you won't have failed pods. Easiest way is to add a simple change like a dummy environment variable in flux-config to re-trigger the release and debug the issue when it occurs.

   - If your old pods are still running when you check, follow [Debug Application Startup issues in AKS](#debug-application-startup-issues-in-aks) to troubleshoot further.

###  Change in git is not applied to cluster

   - To check if latest github commit has been downloaded by checking status

    ```shell
    kubectl get gitrepositories flux-config -n flux-system
    ```
   - If the commit doesn't match latest id, verify source controller logs to see any related errors

    ```shell
    kubectl logs -n flux-system -l app=source-controller
    ```
   - If commit id is recent, verify status of flux kustomization for your namespace to get the version of git applied.

    ```shell
    kubectl get kustomizations.kustomize.toolkit.fluxcd.io -n flux-system <namespace>
    ```
   - If the above status doesn't show latest commit/ show any error , see kustomize controller logs to find relevant errors.

   ```shell
   kubectl logs -n flux-system -l app=kustomize-controller
   # search for specific image
   kubectl logs -n flux-system -l app=kustomize-controller | grep <namespace>
   ```

## Connecting to AKS Clusters

- By Default, all developers have read access to non-prod AKS clusters and slightly higher privileges to their namespaces.
- You can connect to AKS clusters using `az aks get-credentials`. Below are some handy commands:

<%= partial 'clusters' %>

Once you have logged in, you can switch between clusters using [kubectx](https://github.com/ahmetb/kubectx) or below kubectl commands:

```shell
kubectl config use-context cft-perftest-00-aks
kubectl config use-context cft-aat-00-aks
```